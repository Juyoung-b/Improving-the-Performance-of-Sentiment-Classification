# -*- coding: utf-8 -*-
"""sentiment_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nyM6h-0A-_GLoEsaM1Loic70janXX0Z7

# Import requirements
"""

!pip install wandb

!pip install transformers

import os
import pdb
import argparse
from dataclasses import dataclass, field
from typing import Optional
from collections import defaultdict

import torch
from torch.nn.utils.rnn import pad_sequence

import numpy as np
from tqdm import tqdm, trange

from transformers import (
    RobertaForSequenceClassification,
    RobertaTokenizer,
    AutoConfig,
    AdamW
)
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

"""# 1. Make to pickle"""

def make_id_file(task, tokenizer):
    def make_data_strings(file_name):
        data_strings = []
        
        with open(os.path.join('/content/drive/MyDrive/p1_text_classification/p1Data', file_name), 'r', encoding='utf-8') as f:
            id_file_data = [tokenizer.encode(line.lower()) for line in f.readlines()]
        for item in id_file_data:
            data_strings.append(' '.join([str(k) for k in item]))
        return data_strings
    
    print('it will take some times...')
    train_pos = make_data_strings('sentiment.train.1')
    train_neg = make_data_strings('sentiment.train.0')
    dev_pos = make_data_strings('sentiment.dev.1')
    dev_neg = make_data_strings('sentiment.dev.0')

    print('make id file finished!')
    return train_pos, train_neg, dev_pos, dev_neg
#데이터는 총 3가지로 나누어져 있음 train, dev(validation), test(test_no_label.csv)
#1=긍정, 0=부정으로 라벨링

tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

#train_pos, train_neg, dev_pos, dev_neg = make_id_file('yelp', tokenizer)

"""위의 셀을 실행시키지 말고 아래 파일을 실행시키세요

"""

import pickle

results = make_id_file('yelp', tokenizer)
filenames = ['train_pos', 'train_neg', 'dev_pos', 'dev_neg']

for i, v in enumerate(results):
  filename = f"/content/drive/MyDrive/p1_text_classification/p1Data/{filenames[i]}"
  with open(filename, 'wb') as f:
    pickle.dump(v, f)

# 구글드라이브에 토큰된 데이터가 저장되었습니다.    이 과정을 한번만 하면됩니다.

# 구글 드라이브에 저장된 데이터를 불러옵니다.매번 이런식으로 불러오면 시간이 절약됩니다.
filenames = ['train_pos', 'train_neg', 'dev_pos', 'dev_neg']

temp=[]
for filename in filenames:
  with open(f"/content/drive/MyDrive/p1_text_classification/p1Data/{filename}",'rb') as f:
    temp.append(pickle.load(f))

train_pos, train_neg, dev_pos, dev_neg = temp[0], temp[1], temp[2], temp[3]

#train_pos[:10]

# sample_sent=dev_neg[1].split()
# sent = [int(i) for i in sample_sent]
# print(sent)
# decoded = [tokenizer.decode(a) for a in sent]
# print(decoded)

"""# Preprocessing"""

class SentimentDataset(object):
    def __init__(self, tokenizer, pos, neg):
        self.tokenizer = tokenizer
        self.data = []#토큰화된 문장
        self.label = []#레이블(1혹은 0)

        for pos_sent in pos:#긍정인 문장에는 1레이블 붙이기
            self.data += [self._cast_to_int(pos_sent.strip().split())]
            self.label += [[1]]
        for neg_sent in neg:#부정인 문장에는 0 레이블 붙이기
            self.data += [self._cast_to_int(neg_sent.strip().split())]
            self.label += [[0]]

    def _cast_to_int(self, sample):
        return [int(word_id) for word_id in sample]

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        sample = self.data[index]
        return np.array(sample), np.array(self.label[index])

train_dataset = SentimentDataset(tokenizer, train_pos, train_neg)
dev_dataset = SentimentDataset(tokenizer, dev_pos, dev_neg)

def collate_fn_style(samples):
    input_ids, labels = zip(*samples)
    max_len = max(len(input_id) for input_id in input_ids)
    sorted_indices = np.argsort([len(input_id) for input_id in input_ids])[::-1]
    attention_mask = torch.tensor(
        [[1] * len(input_ids[index]) + [0] * (max_len - len(input_ids[index])) for index in
         sorted_indices])
    input_ids = pad_sequence([torch.tensor(input_ids[index]) for index in sorted_indices],batch_first=True)

    
    token_type_ids = torch.tensor([[0] * len(input_ids[index]) for index in sorted_indices])
    position_ids = torch.tensor([list(range(len(input_ids[index]))) for index in sorted_indices])
    labels = torch.tensor(np.stack(labels, axis=0)[sorted_indices])#레이블도 똑같이 소팅

    return input_ids, attention_mask, token_type_ids, position_ids, labels

import wandb
wandb.login()
#https://wandb.ai/authorize

import math

sweep_config = {
    'name' : 'bayes-test',
    'method': 'random',
    'metric' : {
        'name': 'valid_loss',
        'goal': 'minimize'   
        },
    'parameters' : {
        #'optimizer': {
        #    'values': ['adam']
        #    },
        #'dropout': {
        #    'values': [0.3, 0.4]
        #    },
        'learning_rate': {
            'distribution': 'uniform',
            'min': 0,
            'max': 3e-5
            },
        'epochs': {
            'values': [2,3]
            },
        'batch_size': {
            'distribution': 'q_log_uniform',
            'q': 1,
            'min': math.log(64),
            'max': math.log(256),
            }
        }
    }

import torch.optim as optim
import random

def compute_acc(predictions, target_labels):
    return (np.array(predictions) == np.array(target_labels)).mean()

def run_sweeep(config=None):
    wandb.init(config=config)
    
    w_config = wandb.config
    
    train_batch_size= w_config.batch_size
    eval_batch_size=w_config.batch_size

    train_loader = torch.utils.data.DataLoader(train_dataset,
                                           batch_size=train_batch_size,
                                           shuffle=True, collate_fn=collate_fn_style,
                                           pin_memory=True, num_workers=2)
    dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=eval_batch_size,
                                         shuffle=False, collate_fn=collate_fn_style,
                                         num_workers=2)

    random_seed=42
    np.random.seed(random_seed)
    torch.manual_seed(random_seed)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    model = RobertaForSequenceClassification.from_pretrained('roberta-base')
    model.to(device)

    model.train()

    wandb.watch(model)
    
    learning_rate = w_config.learning_rate
    optimizer = AdamW(model.parameters(), lr=learning_rate)

    scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer,
                                        lr_lambda=lambda epoch: 0.75 ** epoch,  
                                        last_epoch=-1,
                                        verbose=True)   
    train_epoch = w_config.epochs
    lowest_valid_loss = 9999.
    train_losses = []
    for epoch in range(train_epoch):
      
      with tqdm(train_loader, unit="batch") as tepoch:

        for iteration, (input_ids, attention_mask, token_type_ids, position_ids, labels) in enumerate(tepoch):

          tepoch.set_description(f"Epoch {epoch}")
          input_ids = input_ids.to(device)
          attention_mask = attention_mask.to(device)
          token_type_ids = token_type_ids.to(device)
          position_ids = position_ids.to(device)
          labels = labels.to(device, dtype=torch.long)

          optimizer.zero_grad()

          output = model(input_ids=input_ids,
                           attention_mask=attention_mask,
                           token_type_ids=token_type_ids,
                           position_ids=position_ids,
                           labels=labels)
          losses = []  
          loss = output.loss
          losses.append(loss.item()) 
          loss.backward() 

          optimizer.step()
            
          if iteration != 0 and iteration % int(len(train_loader) / 1000) == 0:
            train_loss_mean = sum(losses) / len(losses)
            wandb.log({"model train loss": train_loss_mean})
            train_losses.append(train_loss_mean) 
                   
          tepoch.set_postfix(loss=loss.item())
          if iteration != 0 and iteration % int(len(train_loader) / 5) == 0:

                # Evaluate the model five times per epoch
            with torch.no_grad():

              model.eval()
              valid_losses = []
              predictions = []
              target_labels = []

              for input_ids, attention_mask, token_type_ids, position_ids, labels in tqdm(dev_loader,
                                                                                                desc='Eval',
                                                                                                position=1,
                                                                                                leave=None):
                
                input_ids = input_ids.to(device)
                attention_mask = attention_mask.to(device)
                token_type_ids = token_type_ids.to(device)
                position_ids = position_ids.to(device)
                labels = labels.to(device, dtype=torch.long)

                output = model(input_ids=input_ids,
                                       attention_mask=attention_mask,
                                       token_type_ids=token_type_ids,
                                       position_ids=position_ids,
                                       labels=labels)

                logits = output.logits
                loss = output.loss
                valid_losses.append(loss.item())


                batch_predictions = [0 if example[0] > example[1] else 1 for example in logits]
                batch_labels = [int(example) for example in labels]

                predictions += batch_predictions
                target_labels += batch_labels

              acc = compute_acc(predictions, target_labels)
              valid_loss = sum(valid_losses) / len(valid_losses)
              wandb.log({'model acc': acc})
              wandb.log({'model val loss': valid_loss})

              if lowest_valid_loss > valid_loss:
                print('Acc for model which have lower valid loss: ', acc)
                torch.save(model.state_dict(), "./pytorch_model.bin")
     
              model.train()

      scheduler.step()

sweep_id = wandb.sweep(sweep_config, project="textclassification_sentiment")
wandb.agent(sweep_id, run_sweeep, count=6)

#import random
#wandb.init(project="textclassification_sentiment",name='roberta_bs643e-5e3' config= sweep_config)
#wandb.watch(model)

print("lr: ", optimizer.param_groups[0]['lr'])

import pandas as pd
test_df = pd.read_csv('/content/drive/MyDrive/p1_text_classification/p1Data/test_no_label.csv')

test_dataset = test_df['Id']

def make_id_file_test(tokenizer, test_dataset):
    data_strings = []
    id_file_data = [tokenizer.encode(sent.lower()) for sent in test_dataset]
    for item in id_file_data:
        data_strings.append(' '.join([str(k) for k in item]))
    return data_strings

test = make_id_file_test(tokenizer, test_dataset)

class SentimentTestDataset(object):
    def __init__(self, tokenizer, test):
        self.tokenizer = tokenizer
        self.data = []

        for sent in test:
            self.data += [self._cast_to_int(sent.strip().split())]

    def _cast_to_int(self, sample):
        return [int(word_id) for word_id in sample]

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        sample = self.data[index]
        return np.array(sample)

test_dataset = SentimentTestDataset(tokenizer, test)

def collate_fn_style_test(samples):
    input_ids = samples
    max_len = max(len(input_id) for input_id in input_ids)
    #sorted_indices = np.argsort([len(input_id) for input_id in input_ids])[::-1]
    #바꾼부분
    sorted_indices = range(len(input_ids))
    attention_mask = torch.tensor(
        [[1] * len(input_ids[index]) + [0] * (max_len - len(input_ids[index])) for index in
         sorted_indices])

    input_ids = pad_sequence([torch.tensor(input_ids[index]) for index in sorted_indices],
                             batch_first=True)
    
    token_type_ids = torch.tensor([[0] * len(input_ids[index]) for index in sorted_indices])
    position_ids = torch.tensor([list(range(len(input_ids[index]))) for index in sorted_indices])

    return input_ids, attention_mask, token_type_ids, position_ids

test_batch_size = 45
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size,
                                          shuffle=False, collate_fn=collate_fn_style_test,
                                          num_workers=2)

with torch.no_grad():
    model.eval()
    predictions = []
    for input_ids, attention_mask, token_type_ids, position_ids in tqdm(test_loader,
                                                                        desc='Test',
                                                                        position=1,
                                                                        leave=None):
        input_ids = input_ids.to(device)
        attention_mask = attention_mask.to(device)
        token_type_ids = token_type_ids.to(device)
        position_ids = position_ids.to(device)

        output = model(input_ids=input_ids,
                       attention_mask=attention_mask,
                       token_type_ids=token_type_ids,
                       position_ids=position_ids)

        logits = output.logits
        batch_predictions = [0 if example[0] > example[1] else 1 for example in logits]
        predictions += batch_predictions

test_df['Category'] = predictions

test_df.to_csv('submission.csv', index=False)